# Optimization of Gradient Descent

## 1 优化算法

### 1.1 随机梯度下降 SGD

1. **特征**：随机梯度下降算法，在当前点计算梯度，根据学习率前进到下一点。到中点附近时，由于样本误差或者学习率问题，会发生来回徘徊的现象，很可能会错过最优解。

2. **输入和参数**：$\eta$ - 全局学习率

3. **算法**：

	> 计算梯度：$g_t = \nabla_\theta J(\theta_{t-1})$
	>
	> 更新参数：$\theta_t = \theta_{t-1}  - \eta \cdot g_t$

4. **潜在的问题**：

	- SGD的一个缺点就是收敛速度慢。如，在学习率为`0.1`时，训练10000个`epoch`不能收敛到预定损失值；学习率为`0.3`时，训练5000个`epoch`可以收敛到预定水平。
	- SGD方法的另一个缺点是其更新方向完全依赖于当前batch计算出的梯度，因而十分不稳定，因为数据有噪音。

### 1.2 动量算法 Momentum

1. Momentum算法借用了物理中的动量概念，它模拟的是物体运动时的惯性，即更新的时候在一定程度上保留之前更新的方向，同时利用当前batch的梯度微调最终的更新方向。

  这样一来，可以在一定程度上增加稳定性，从而学习地更快，并且还有一定摆脱局部最优的能力。Momentum算法会观察历史梯度，若当前梯度的方向与历史梯度一致（表明当前样本不太可能为异常点），则会增强这个方向的梯度。若当前梯度与历史梯度方向不一致，则梯度会衰减。

<img src="../assets/momentum_algorithm.png" width="600">



  > 在图中，第一次的梯度更新完毕后，会记录$v_1$的动量值。在“求梯度点”进行第二次梯度检查时，得到2号方向，与$v_1$的动量组合后，最终的更新为2'方向。这样一来，由于有$v_1$的存在，会迫使梯度更新方向具备“惯性”，从而可以减小随机样本造成的震荡。

2. **输入和参数**：

  - $\eta$ - 全局学习率
  - $\alpha$ - 动量参数，一般取值为0.5, 0.9, 0.99
  - $v_t$ - 当前时刻的动量，初值为0

3. **算法**：

  > 计算梯度：$g_t = \nabla_\theta J(\theta_{t-1})$
  >
  > 更新动量：$v_t = \alpha v_{t-1} - \eta g_t$
  >
  > 更新参数：$\theta_t = \theta_{t-1} + v_t$

